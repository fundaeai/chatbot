#!/usr/bin/env python3
"""
FastAPI server for RAG Retrieval System
Provides REST API endpoints for search and question answering
"""

import logging
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

# Add parent directory to path for proper imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

from core.rag_orchestrator import RAGOrchestrator
from config import config

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def _process_markdown_for_display(raw_markdown: str) -> str:
    """Convert raw markdown to a readable format for display"""
    
    # Replace markdown syntax with readable text
    processed = raw_markdown
    
    # Headers
    processed = processed.replace('# ', 'ðŸ“‹ ')
    processed = processed.replace('## ', 'ðŸ“– ')
    processed = processed.replace('### ', 'ðŸ“ ')
    processed = processed.replace('#### ', 'ðŸ“„ ')
    
    # Bold text
    processed = processed.replace('**', '')
    
    # Code blocks
    processed = processed.replace('`', '')
    
    # Horizontal rules
    processed = processed.replace('---', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”')
    
    # Bullet points
    processed = processed.replace('- ', 'â€¢ ')
    
    # Blockquotes
    processed = processed.replace('> ', 'ðŸ’¡ ')
    
    # Links (simplify)
    processed = processed.replace('[', '').replace(']', '')
    
    return processed

def _generate_markdown_response(question: str, answer: str, sources: List[Dict[str, Any]], 
                               confidence: float, processing_time: float, search_type: str) -> str:
    """Generate a complete markdown file with metadata and formatted content"""
    
    # Get current timestamp
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Generate markdown content
    markdown_content = f"""# RAG System Response

## ðŸ“‹ Query Information
- **Question:** {question}
- **Generated:** {timestamp}
- **Search Type:** {search_type}
- **Processing Time:** {processing_time:.2f} seconds
- **Confidence Score:** {confidence:.1%}

---

## ðŸ¤– AI Response

{answer}

---

## ðŸ“š Sources Used

This response was generated using the following {len(sources)} document sources:

"""
    
    # Add sources with formatting
    for i, source in enumerate(sources, 1):
        markdown_content += f"""### Source {i}: {source['filename']}
- **Page:** {source['page']}
- **Relevance Score:** {source['score']:.3f}
- **Content Type:** {source['chunk_type']}
- **Content Preview:** {source['content'][:200]}...

"""
    
    # Add footer
    markdown_content += f"""
---

## ðŸ“Š System Information
- **RAG System:** GPT-4.1 with Hybrid Search
- **Document Count:** {len(sources)} sources
- **Response Quality:** {'High' if confidence > 0.7 else 'Medium' if confidence > 0.4 else 'Low'} confidence
- **Generated by:** RAG2.0 System

---
*This response was automatically generated by the RAG (Retrieval-Augmented Generation) system.*
"""
    
    return markdown_content

# Initialize FastAPI app
app = FastAPI(
    title="RAG Retrieval System API",
    description="API for intelligent document search and question answering",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global service instances
rag_orchestrator = None

# Pydantic models
class SearchRequest(BaseModel):
    query: str
    top_k: Optional[int] = None
    search_type: Optional[str] = "hybrid"
    min_score: Optional[float] = None

class SearchResponse(BaseModel):
    results: List[Dict[str, Any]]
    total_results: int
    search_time: float
    search_type: str

class QuestionRequest(BaseModel):
    question: str
    # Optional parameters with sensible defaults
    context_length: Optional[int] = 4000
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 500
    search_type: Optional[str] = "hybrid"
    top_k: Optional[int] = 5

class QuestionResponse(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]
    confidence: float
    processing_time: float
    search_results_count: int
    context_length: Optional[int] = None
    search_type: str

class HealthResponse(BaseModel):
    status: str
    services: Dict[str, Any]
    config: Dict[str, Any]

@app.on_event("startup")
async def startup_event():
    """Initialize the RAG orchestrator on startup"""
    global rag_orchestrator
    
    try:
        # Validate configuration
        config.Config.validate_config()
        
        # Initialize RAG orchestrator
        rag_orchestrator = RAGOrchestrator()
        
        logger.info("RAG orchestrator initialized successfully")
        
    except Exception as e:
        logger.error(f"Failed to initialize RAG orchestrator: {e}")
        raise

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Check system health and configuration"""
    try:
        # Get pipeline statistics
        pipeline_stats = rag_orchestrator.get_pipeline_statistics() if rag_orchestrator else {}
        
        # Get configuration summary
        config_summary = config.Config.get_config_summary()
        
        return HealthResponse(
            status="healthy",
            services=pipeline_stats,
            config=config_summary
        )
        
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search", response_model=SearchResponse)
async def search_documents(request: SearchRequest):
    """Search for relevant documents (Step 1: Retrieval)"""
    try:
        start_time = time.time()
        
        # Use RAG orchestrator for search-only
        results = rag_orchestrator.search_only(
            query=request.query,
            top_k=request.top_k or 5,
            search_type=request.search_type
        )
        
        search_time = time.time() - start_time
        
        return SearchResponse(
            results=results,
            total_results=len(results),
            search_time=search_time,
            search_type=request.search_type
        )
        
    except Exception as e:
        logger.error(f"Search error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/ask", response_model=QuestionResponse)
async def ask_question(request: QuestionRequest):
    """Ask a question and get AI-generated answer (Complete RAG Pipeline)"""
    try:
        # Use RAG orchestrator for complete pipeline with sensible defaults
        result = rag_orchestrator.ask(
            question=request.question,
            top_k=request.top_k,
            search_type=request.search_type,
            context_length=request.context_length,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        
        return QuestionResponse(
            answer=result['answer'],
            sources=result['sources'],
            confidence=result['confidence'],
            processing_time=result['processing_time'],
            search_results_count=len(result['sources']),
            context_length=result.get('context_length'),
            search_type=result.get('search_type', 'hybrid')
        )
        
    except Exception as e:
        logger.error(f"Question answering error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/statistics")
async def get_statistics():
    """Get system statistics"""
    try:
        pipeline_stats = rag_orchestrator.get_pipeline_statistics() if rag_orchestrator else {}
        
        return {
            "pipeline": pipeline_stats,
            "config": config.Config.get_config_summary()
        }
        
    except Exception as e:
        logger.error(f"Statistics error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
async def root():
    """Root endpoint with API information"""
    return {
        "message": "RAG Retrieval System API",
        "version": "1.0.0",
        "description": "Ask questions and get intelligent answers based on your documents",
        "endpoints": {
            "ask_question": "/ask",
            "search_documents": "/search",
            "health_check": "/health",
            "statistics": "/statistics"
        },
        "documentation": "/docs",
        "usage": {
            "simple": "Just send your question: {\"question\": \"Your question here\"}",
            "example": {
                "question": "What is machine learning?",
                "context_length": 4000,
                "temperature": 0.7,
                "max_tokens": 500,
                "search_type": "hybrid",
                "top_k": 5
            }
        }
    }

if __name__ == "__main__":
    import time
    uvicorn.run(
        "rag_api:app",
        host=config.Config.API_HOST,
        port=config.Config.API_PORT,
        reload=True,
        log_level="info"
    ) 